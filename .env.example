# LLM Provider
LLM_DEFAULT_PROVIDER=groq                  # groq | ollama
LLM_FALLBACK_PROVIDER=ollama               # optional fallback if default fails

# Groq API
GROQ_API_KEY=gsk_your_api_key_here
GROQ_MODEL=llama-3.1-8b-instant
GROQ_REVIEW_MODEL=llama-3.3-70b-versatile  # Optional: larger model for reviews

# Ollama (self-hosted alternative)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_API_KEY=your_api_key_if_required
OLLAMA_API_URL=https://ollama.foodshare.club/api/chat
OLLAMA_TRANSLATE_API_URL=https://translate.foodshare.club

# Cloudflare Access (for translation service)
CF_ACCESS_CLIENT_ID=your_client_id
CF_ACCESS_CLIENT_SECRET=your_client_secret

# Translation Service
TRANSLATE_API_KEYS=comma_separated_api_keys

# Redis HTTP Wrapper
REDIS_HTTP_TOKEN=your-secure-token

# GitHub Integration
GITHUB_TOKEN=ghp_your_personal_access_token
GITHUB_WEBHOOK_SECRET=your_webhook_secret

# Supabase
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

# App URL (for edge functions to call back)
APP_URL=https://your-app.vercel.app
NEXT_PUBLIC_APP_URL=https://your-app.vercel.app

# Cron/Worker Auth (use strong random string)
CRON_SECRET=your_random_secret_here_min_32_chars

# Notifications (optional)
SLACK_WEBHOOK_URL=
DISCORD_WEBHOOK_URL=

# Environment
NODE_ENV=production
